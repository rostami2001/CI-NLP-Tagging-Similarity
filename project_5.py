# -*- coding: utf-8 -*-
"""Copy of project_5_lastVersion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BwZWqYD6ZnB4I45q3fSvtIC4CBvvVOVH
"""

from google.colab import drive
drive.mount('/content/drive')

"""# imports"""

import re
import nltk
import random
import string
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from nltk.stem import WordNetLemmatizer
from mpl_toolkits.mplot3d import Axes3D
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from sklearn.metrics import accuracy_score
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity

"""# downloads"""

# Download NLTK datasets
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download('punkt_tab')

"""# paths

paths to the dataset files
"""

train_file_path =  "/content/drive/MyDrive/project_5/dataset/train.csv"
validation_file_path = "/content/drive/MyDrive/project_5/dataset/valid.csv"

"""# **phase (1)**

lemmatizer: to convert words to their base form

stop_words: a set of meaningless words to be removed
"""

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

"""cleans the text (removing HTML, numbers, punctuation,...)"""

def clean_text(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    text = re.sub(r"[^a-zA-Z]", " ", text)
    text = re.sub(r'\d+', '', text)
    text = text.lower()
    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))
    translator = str.maketrans('', '', string.punctuation)
    text = text.translate(translator)
    text = " ".join(text.split())
    words = word_tokenize(text)
    words = [word for word in words if word not in stop_words]
    words = [lemmatizer.lemmatize(word) for word in words]
    return " ".join(words)

"""loads the dataset and cleans its texts."""

def preprocess_dataset(file_path):
    df = pd.read_csv(file_path)
    df["Title"] = df["Title"].apply(lambda x: clean_text(x))
    df["Body"] = df["Body"].apply(lambda x: clean_text(x))
    df["Tags"] = df["Tags"].apply(lambda x: x.replace("<", "").replace(">", " ").strip())
    df.dropna(subset=["Title", "Body", "Tags"], inplace=True)
    df = df[["Title", "Body", "Tags"]]

    return df

"""call functions"""

train_data = preprocess_dataset(train_file_path)
validation_data = preprocess_dataset(validation_file_path)

"""saved for use in later processing steps"""

train_data.to_csv("/content/drive/MyDrive/project_5/dataset/clean_train_dataset.csv", index=False)
validation_data.to_csv("/content/drive/MyDrive/project_5/dataset/clean_validation_dataset.csv", index=False)

"""# **phase (2)**

train_word2vec -> Function to train Word2Vec model

compute_sentence_vector -> Function to compute sentence vector

visualize_word_vectors -> Function to visualize word vectors

visualize_document_vectors -> Function to visualize document vectors

calculate_cosine_similarity -> Function to calculate cosine similarity

find_similar_questions -> Function to find similar questions
"""

def train_word2vec(corpus):
    model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=5, workers=4)
    model.train(corpus, total_examples=len(corpus), epochs=40)
    return model

def compute_sentence_vector(words, model):
    valid_words = [model.wv[word] for word in words if word in model.wv]
    if len(valid_words) > 0:
        vector = np.mean(valid_words, axis=0)
    else:
        vector = np.zeros(model.vector_size)
    return vector

def visualize_word_vectors(model):
    words = random.sample(model.wv.index_to_key, min(50, len(model.wv.index_to_key)))
    vectors = np.array([model.wv[word] for word in words])

    pca = PCA(n_components=3)
    reduced_vectors = pca.fit_transform(vectors)

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    for i, word in enumerate(words):
        ax.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2])
        ax.text(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2], word)

    plt.title("Word Vectors Visualization")
    plt.show()

def visualize_document_vectors(data, model):
    document_vectors = np.array(data["SemanticVector"].tolist())

    pca = PCA(n_components=3)
    reduced_vectors = pca.fit_transform(document_vectors)

    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')

    for i in range(min(len(reduced_vectors), 100)):
        ax.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2])
        ax.text(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2], f"Doc-{i+1}")

    plt.title("Document Vectors Visualization")
    plt.show()

def calculate_cosine_similarity(vector1, vector2):
    return cosine_similarity([vector1], [vector2])[0][0]

def find_similar_questions(query, train_data, word2vec_model, top_n=5):
    query_tokens = clean_text(query)
    query_tokens = word_tokenize(query)
    query_vector = compute_sentence_vector(query_tokens, word2vec_model)

    if query_vector is None or not query_vector.any():
        print("Query vector could not be generated. Please check the input.")
        return []

    similarities = []
    for i, row in train_data.iterrows():
        doc_vector = row["SemanticVector"]
        similarity = calculate_cosine_similarity(query_vector, doc_vector)
        similarities.append((i, similarity))

    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)

    similar_questions = []
    for idx, similarity in similarities[:top_n]:
        title = train_data.iloc[idx]["Title"]
        similar_questions.append((title, similarity))

    return similar_questions

"""Drop rows with empty Title or Body , Convert Title and Body to string ,Tokenize Title and Body"""

train_data.dropna(subset=["Title", "Body"], inplace=True)

train_data["Title"] = train_data["Title"].astype(str)
train_data["Body"] = train_data["Body"].astype(str)

train_data["TokenizedTitle"] = train_data["Title"].apply(lambda x: word_tokenize(x))
train_data["TokenizedBody"] = train_data["Body"].apply(lambda x: word_tokenize(x))

"""Prepare the corpus"""

corpus = train_data["TokenizedTitle"].tolist() + train_data["TokenizedBody"].tolist()

"""train the Word2Vec model"""

word2vec_model = train_word2vec(corpus)

"""Compute semantic vectors for each document"""

train_data["SemanticVector"] = train_data.apply(
    lambda row: compute_sentence_vector(row["TokenizedTitle"] + row["TokenizedBody"], word2vec_model), axis=1
)

"""visualize word vectors"""

visualize_word_vectors(word2vec_model)

"""visualize document vectors"""

visualize_document_vectors(train_data, word2vec_model)

"""get Query"""

query = input("Enter your Question: ")

"""Finding questions similar to an query"""

similar_questions = find_similar_questions(query, train_data, word2vec_model, top_n=5)

"""similar questions with similarity"""

print("Similar Questions:")
for i, (title, similarity) in enumerate(similar_questions, start=1):
    print(f"{i}. {title} (Similarity: {similarity:.2f})")

"""# phase (3)

preprocess_validation_data -> Function to prepare validation data

train_knn_model -> Function to train KNN model

predict_tags -> Function to predict tags

evaluate_predictions -> Function to evaluate

show_examples -> Function to show exaples
"""

def preprocess_validation_data(validation_data, word2vec_model):
    validation_data = validation_data.sample(frac=0.1, random_state=42)
    validation_data["Title"] = validation_data["Title"].apply(lambda x: clean_text(x))
    validation_data["Body"] = validation_data["Body"].apply(lambda x: clean_text(x))
    validation_data["TokenizedTitle"] = validation_data["Title"].apply(lambda x: word_tokenize(x))
    validation_data["TokenizedBody"] = validation_data["Body"].apply(lambda x: word_tokenize(x))
    validation_data["SemanticVector"] = validation_data.apply(
        lambda row: compute_sentence_vector(row["TokenizedTitle"] + row["TokenizedBody"], word2vec_model), axis=1
    )

    return validation_data

def train_knn_model(train_vectors, k=5):
    knn = NearestNeighbors(n_neighbors=k, metric="cosine")
    knn.fit(train_vectors)
    return knn

def predict_tags(knn, validation_vectors, train_data, k=5):
    predictions = []
    for vector in validation_vectors:
        distances, indices = knn.kneighbors([vector])
        neighbor_tags = train_data.iloc[indices[0]]["Tags"].tolist()
        flat_tags = [tag for tags in neighbor_tags for tag in tags.split()]
        unique_tags = list(set(flat_tags))
        predictions.append(unique_tags)
    return predictions

def evaluate_predictions(predicted_tags, true_tags):
    correct_predictions = 0
    for pred, true in zip(predicted_tags, true_tags):
        if any(tag in true.split() for tag in pred):
            correct_predictions += 1
    accuracy = correct_predictions / len(predicted_tags)
    return accuracy

def show_examples(validation_data, predicted_tags):

    for i in range(5):
        idx = random.randint(0, len(validation_data) - 1)
        pred = predicted_tags[idx]
        true = validation_data.iloc[idx]['Tags'].split()
        is_correct = any(tag in true for tag in pred)
        print(f"Example {i+1}:")
        print(f"Title: {validation_data.iloc[idx]['Title']}")
        print(f"True Tags: {', '.join(true)}")
        print(f"Predicted Tags: {', '.join(pred)}")
        print(f"Prediction Success: {'✅' if is_correct else '❌'}")
        print("-" * 50)

"""validation data preprocess"""

validation_data = preprocess_validation_data(validation_data, word2vec_model)

train_data["TokenizedTitle"] = train_data["Title"].apply(lambda x: word_tokenize(x))
train_data["TokenizedBody"] = train_data["Body"].apply(lambda x: word_tokenize(x))
corpus = train_data["TokenizedTitle"].tolist() + train_data["TokenizedBody"].tolist()

word2vec_model = train_word2vec(corpus)

train_data["SemanticVector"] = train_data.apply(
    lambda row: compute_sentence_vector(row["TokenizedTitle"] + row["TokenizedBody"], word2vec_model), axis=1
)

"""prepare train data and validation data"""

train_vectors = np.array(train_data["SemanticVector"].tolist())
validation_vectors = np.array(validation_data["SemanticVector"].tolist())

"""train KNN model"""

knn_model = train_knn_model(train_vectors, k=5)

"""predicted tags"""

predicted_tags = predict_tags(knn_model, validation_vectors, train_data, k=5)

"""Model accuracy"""

true_tags = validation_data["Tags"].tolist()
accuracy = evaluate_predictions(predicted_tags, true_tags)
print(f"Model Accuracy: {accuracy:.2f}")

"""show example"""

show_examples(validation_data, predicted_tags)